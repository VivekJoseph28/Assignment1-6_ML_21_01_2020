{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: Machine Learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What are the three stages to build the hypotheses or model in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial steps involved in machine learning are as follows\n",
    "\n",
    "Step 1:  Data Selection :-\n",
    "\n",
    "To obtain/pull  structured data from a database/ files to process it for building the machine learning model. The headers, the index and the time frames need to be in the aligned in the required format through data pulls from a data base or arranged in the excel files.\n",
    "\n",
    "\n",
    "Step 2:  Data Preprocessing:-\n",
    "\n",
    "Formatting: The data you have selected may not be in a format that is suitable for you to work with. The data may be in a relational database and you would like it in a flat file, or the data may be in a proprietary file format and you would like it in a relational database or a text file. Cleaning: Cleaning data is the removal or fixing of missing data. There may be data instances that are incomplete and do not carry the data you believe you need to address the problem. These instances may need to be removed. Additionally, there may be sensitive information in some of the attributes and these attributes may need to be anonymized or removed from the data entirely. Sampling: There may be far more selected data available than you need to work with. More data can result in much longer running times for algorithms and larger computational and memory requirements. You can take a smaller representative sample of the selected data that may be much faster for exploring and prototyping solutions before considering the whole dataset. It is very likely that the machine learning tools you use on the data will influence the preprocessing you will be required to perform. You will likely revisit this step.\n",
    "\n",
    "Step 3: Transform Data:-\n",
    "\n",
    "Three common data transformations are scaling, attribute decompositions and attribute aggregations. This step is also referred to as feature engineering.\n",
    "•Scaling: The preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. Many machine learning methods like data attributes to have the same scale such as between 0 and 1 for the smallest and largest value for a given feature. Consider any feature scaling you may need to perform.\n",
    "•Decomposition: There may be features that represent a complex concept that may be more useful to a machine learning method when split into the constituent parts. An example is a date that may have day and time components that in turn could be split out further. Perhaps only the hour of day is relevant to the problem being solved. consider what feature decompositions you can perform.\n",
    "•Aggregation: There may be features that can be aggregated into a single feature that would be more meaningful to the problem you are trying to solve. For example, there may be a data instances for each time a customer logged into a system that could be aggregated into a count for the number of logins allowing the additional instances to be discarded. Consider what type of feature aggregations could perform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is the standard approach to supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning algorithms are trained using labeled data as an input where the desired output is known. \n",
    "For example, a piece of equipment could have data points labeled either “F” (failed) or “R” (runs).\n",
    "The learning algorithm receives a set of inputs along with the corresponding correct outputs, and the algorithm learns\n",
    "by comparing its actual output with correct outputs to find errors. It then modifies the model accordingly. \n",
    "Through methods like classification, regression, prediction and gradient boosting, supervised learning \n",
    "uses patterns to predict the values of the label on additional unlabeled data. \n",
    "Supervised learning is commonly used in applications where historical data predicts likely future events.\n",
    "For example, it can anticipate when credit card transactions are likely to be fraudulent or which insurance customer\n",
    "is likely to file a claim. \n",
    "\n",
    "Supervised learinng algorithms are of the following types:\n",
    "    \n",
    "1.Classification. When the data are being used to predict a category, supervised learning is also called classification. This is the case when assigning an image as a picture of either a 'cat' or a 'dog'. When there are only two choices, it's called two-class or binomial classification. When there are more categories, as when predicting the winner of the NCAA March Madness tournament, this problem is known as multi-class classification.\n",
    "\n",
    "2.Regression. When a value is being predicted, as with stock prices, supervised learning is called regression.\n",
    "\n",
    "3.Anomaly detection. Sometimes the goal is to identify data points that are simply unusual. \n",
    "In fraud detection, for example, any highly unusual credit card spending patterns are suspect. The possible variations are so numerous and the training examples so few, that it's not feasible to learn what fraudulent activity looks like. The approach that anomaly detection takes is to simply learn what normal activity looks like (using a history non-fraudulent transactions) and identify anything that is significantly different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is Training set and Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General thumb rule is the existing data set is split into train and test data using 80/20 or 75/25 \n",
    "rule based on data point availability. Cross validation/k-fold validation can also be used to shuffle the data sets \n",
    "and improve the accuracy and model fit.\n",
    "\n",
    "Training Data Set :- In  machine learning problem, a training set is a dataset used to train a model. \n",
    "In training the model, specific features are picked out from the training set. \n",
    "These features are then incorporated into the model. Thereby, if the training set is labeled correctly, the model \n",
    "should be able to learn something from these features.\n",
    "\n",
    "testing Data Set:- In machine learning problem, a training set is a dataset used to train a model. \n",
    "In training the model, specific features are picked out from the training set. \n",
    "These features are then incorporated into the model. Thereby, if the training set is labeled correctly, the model\n",
    "should be able to learn something from these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the general principle of an ensemble method and what isbagging and boosting in ensemble method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning helps improve machine learning results by combining several models. \n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one \n",
    "predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n",
    "\n",
    "Bagging:- often considers homogeneous weak learners, learns them independently from each other in parallel\n",
    "and combines them following some kind of deterministic averaging process.\n",
    "\n",
    "Boosting:- often considers homogeneous weak learners, learns them sequentially in a very adaptative way \n",
    "(a base model depends on the previous ones) and combines them following a deterministic strategy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it\n",
    "negatively impacts the performance of the model on new data. This means that the noise or random fluctuations\n",
    "in the training data is picked up and learned as concepts by the model. The problem is that these concepts\n",
    "do not apply to new data and negatively impact the models ability to generalize.\n",
    "\n",
    "Overfitting can be avoided by using the following techniques:\n",
    "\n",
    "1. Cross-validation:- \n",
    "Cross-validation is a powerful preventative measure against overfitting.\n",
    "\n",
    "The idea is clever: Use your initial training data to generate multiple mini train-test splits. \n",
    "Use these splits to tune your model.\n",
    "\n",
    "In standard k-fold cross-validation, we partition the data into k subsets, called folds.\n",
    "Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set \n",
    "(called the “holdout fold”).\n",
    "\n",
    "2.Train with more data:-\n",
    "It won’t work every time, but training with more data can help algorithms detect the signal better. \n",
    "In the earlier example of modeling height vs. age in children, it’s clear how sampling more schools \n",
    "will help your model.\n",
    "\n",
    "3.Remove features :-\n",
    "Some algorithms have built-in feature selection.\n",
    "For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n",
    "An interesting way to do so is to tell a story about how each feature fits into the model. \n",
    "This is like the data scientist's spin on software engineer’s rubber duck debugging technique, \n",
    "where they debug their code by explaining it, line-by-line, to a rubber duck.\n",
    "\n",
    "4.Early stopping:-\n",
    "When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n",
    "Up until a certain number of iterations, new iterations improve the model. \n",
    "After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "Early stopping refers stopping the training process before the learner passes that point.\n",
    "\n",
    "\n",
    "5.Regularization:-\n",
    "Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n",
    "\n",
    "The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "Oftentimes, the regularization method is a hyperparameter as well, which means it can be tuned through cross-validation.\n",
    "\n",
    "6.Ensembling:-\n",
    "Ensembles are machine learning methods for combining predictions from multiple separate models. \n",
    "There are a few different methods for ensembling, but the two most common are:\n",
    "\n",
    "Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "It trains a large number of \"strong\" learners in parallel.\n",
    "A strong learner is a model that's relatively unconstrained.\n",
    "Bagging then combines all the strong learners together in order to \"smooth out\" their predictions.\n",
    "\n",
    "Boosting attempts to improve the predictive flexibility of simple models.\n",
    "\n",
    "It trains a large number of \"weak\" learners in sequence.\n",
    "A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).\n",
    "Each one in the sequence focuses on learning from the mistakes of the one before it.\n",
    "Boosting then combines all the weak learners into a single strong learner.\n",
    "\n",
    "While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#----------------------------------------------------------#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
